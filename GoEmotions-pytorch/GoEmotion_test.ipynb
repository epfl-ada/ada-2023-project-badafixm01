{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc4712d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from model import BertForMultiLabelClassification\n",
    "from multilabel_pipeline import MultiLabelPipeline\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pandas as pd\n",
    "import ast\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Requirements:\n",
    "# create environment with python=3.8 or 3.7 (required for tokenizers 0.7.0 below)\n",
    "# conda install pytorch==1.4.0 cpuonly -c pytorch \n",
    "# conda install -c scw torchvision \n",
    "# conda install -c conda-forge attrdict  \n",
    "# from: https://pypi.org/project/tokenizers/0.7.0/#files download wheel for tokenizer==0.7.0 (required for transformers=2.11.0) -- run command pip install tokenizers-0.7.0-cp38-cp38-win_amd64.whl (with appropiate wheel file name)\n",
    "# pip install transformers==2.11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc42779",
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3aa9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the text file\n",
    "with open(R\"C:\\Users\\Berta\\Desktop\\EPFL\\ADA\\PROJECT\\MovieSummaries\\plot_summaries.txt\", 'r', encoding='utf-8') as file:    \n",
    "    content_summaries = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310daf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the content into individual summaries based on \"movie_id\"\n",
    "summaries = content_summaries.split('\\n')\n",
    "\n",
    "# Remove empty lines if any\n",
    "plot_summaries = [summary.strip() for summary in summaries if summary.strip()]\n",
    "\n",
    "movie_ids = []\n",
    "summary_texts = []\n",
    "\n",
    "# Split the plot summaries into movie IDs and summary text\n",
    "for summary in plot_summaries:\n",
    "    parts = summary.split('\\t', 1)  # Split at the first space character\n",
    "    if len(parts) == 2:\n",
    "        movie_id, summary_text = parts\n",
    "        movie_ids.append(int(movie_id))\n",
    "        summary_texts.append(summary_text)\n",
    "\n",
    "summaries_df = pd.DataFrame({'Movie ID': movie_ids, 'Plot': summary_texts})\n",
    "\n",
    "column_names = [\n",
    "    'Movie ID',\n",
    "    'Freebase ID',\n",
    "    'Movie Title',\n",
    "    'Release Date',\n",
    "    'Box Office',\n",
    "    'Runtime',\n",
    "    'Language',\n",
    "    'Country',\n",
    "    'Genre'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(R\"C:\\Users\\Berta\\Desktop\\EPFL\\ADA\\PROJECT\\MovieSummaries\\movie.metadata.tsv\", delimiter='\\t', header=None, names=column_names)\n",
    "df = df[~(df['Genre']=='{}').values]\n",
    "\n",
    "def extract_first_genre(genre_str):\n",
    "    genre_dict = ast.literal_eval(genre_str)\n",
    "    return next(iter(genre_dict.values()))\n",
    "\n",
    "df['Genre'] = df['Genre'].apply(extract_first_genre)\n",
    "df['Language'] = df['Language'].apply(lambda x: ', '.join([value.split()[0] for key, value in ast.literal_eval(x).items()]))\n",
    "df['Country'] = df['Country'].apply(lambda x: ', '.join([value for key, value in ast.literal_eval(x).items()]))\n",
    "\n",
    "metadata = df[df['Movie ID'].isin(movie_ids)]\n",
    "summaries_df = summaries_df[summaries_df['Movie ID'].isin(metadata['Movie ID'])]\n",
    "\n",
    "movie_data = pd.merge(metadata, summaries_df, on='Movie ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for tokenisation:\n",
    "\n",
    "# Note: same summary and title for this entries, but different release year! They also cause problems with manual tokenisation, so they will be removed for the moment. Further analysis needed.\n",
    "# movie_data.iloc[9724]\n",
    "# movie_data.iloc[25327]\n",
    "# movie_data.iloc[29447]\n",
    "# movie_data.iloc[33197]\n",
    "\n",
    "# Other entries which caused problems with tokenizer:\n",
    "# movie_data.iloc[16550]\n",
    "# movie_data.iloc[29414]\n",
    "# movie_data.iloc[30229]\n",
    "# movie_data.iloc[34493]\n",
    "\n",
    "plots_need_analsys = movie_data.iloc[[9724, 16550, 25327, 29414, 29447, 30229, 33197, 34493]]\n",
    "movie_data = movie_data.drop([9724, 16550, 25327, 29414, 29447, 30229, 33197, 34493])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only movie data for 10 most recurrent Genres:\n",
    "genre_occurrences = movie_data['Genre'].value_counts()\n",
    "top_10_genres = genre_occurrences[:10].index.to_numpy()\n",
    "movie_data_top10 = movie_data[movie_data['Genre'].isin(top_10_genres)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained GoEmotions model and tokenizer\n",
    "model_name = \"monologg/bert-base-cased-goemotions-original\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Berta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Manual tokenization with max_len=512 to avoid sentences splitting in half and allow mapping emotions to each plot summary:\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def sentence_aware_split(text, max_length, tokenizer):\n",
    "    # Split text into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        # Check if adding this sentence would exceed the max_length\n",
    "        potential_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "        potential_chunk_tokenized = tokenizer.tokenize(potential_chunk)\n",
    "        \n",
    "        if len(potential_chunk_tokenized) <= max_length - 2:  # -2 for [CLS] and [SEP]\n",
    "            current_chunk = potential_chunk\n",
    "        else:\n",
    "            # Add the current_chunk to chunks and start a new one\n",
    "            chunks = chunks + [current_chunk]\n",
    "            current_chunk = sentence\n",
    "\n",
    "    # Don't forget to add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks = chunks + [current_chunk]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "max_length = 512  # Adjust based on your model's max length\n",
    "\n",
    "texts = movie_data['Plot'].values.tolist()\n",
    "tokenized = []\n",
    "chunk_to_text_mapping = {}\n",
    "for i, text in enumerate(texts):\n",
    "    \n",
    "    chunked_text = sentence_aware_split(text, max_length, tokenizer)\n",
    "\n",
    "    for chunk in chunked_text:\n",
    "        chunk_to_text_mapping[chunk] = i\n",
    "\n",
    "    tokenized = tokenized + chunked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary chunks and chunks to plot mapping for future runs:\n",
    "\n",
    "def save_list_to_file(list_of_strings, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for i, string in enumerate(list_of_strings):\n",
    "            file.write(string + '\\n')\n",
    "\n",
    "def save_dict_to_file(dictionary, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for key, value in dictionary.items():\n",
    "            file.write(f'{key}: {value}\\n')\n",
    "\n",
    "save_list_to_file(tokenized, R'C:\\Users\\Berta\\Desktop\\EPFL\\ADA\\PROJECT\\ada-2023-project-badafixm01\\GoEmotions-pytorch\\tokenized_plots.txt')\n",
    "save_dict_to_file(chunk_to_text_mapping, R'C:\\Users\\Berta\\Desktop\\EPFL\\ADA\\PROJECT\\ada-2023-project-badafixm01\\GoEmotions-pytorch\\chunk_mapping.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5576cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to load files to avoid re-running manual tokenisation cell:\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open(R\"C:\\Users\\Berta\\Desktop\\EPFL\\ADA\\PROJECT\\ada-2023-project-badafixm01\\GoEmotions-pytorch\\final\\tokenized_plots.txt\", 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "tokenized = file_content.strip().split('\\n')\n",
    "\n",
    "with open(R\"C:\\Users\\Berta\\Desktop\\EPFL\\ADA\\PROJECT\\ada-2023-project-badafixm01\\GoEmotions-pytorch\\final\\chunk_mapping.txt\", 'r', encoding='utf-8') as file:\n",
    "    chunk_mapping = file.read()\n",
    "\n",
    "# Split the content into individual summaries based on \"movie_id\"\n",
    "chunk_to_text_mapping = chunk_mapping.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Berta\\anaconda3\\envs\\ada_goEm\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "goemotions = pipeline(\n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        task=\"text-classification\",\n",
    "        return_all_scores=True,\n",
    "        function_to_apply='sigmoid',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only 76 sequences for first analysis\n",
    "batch_size = 10  # or another size that suits your system\n",
    "batches = [tokenized[i:i + batch_size] for i in range(0, len(tokenized[:(batch_size*10)]), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0171efb6474049d1aa8730fde400f8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/10 processed successfully.\n",
      "Batch 2/10 processed successfully.\n",
      "Batch 3/10 processed successfully.\n",
      "Batch 4/10 processed successfully.\n",
      "Batch 5/10 processed successfully.\n",
      "Batch 6/10 processed successfully.\n",
      "Batch 7/10 processed successfully.\n",
      "Batch 8/10 processed successfully.\n",
      "Batch 9/10 processed successfully.\n",
      "Batch 10/10 processed successfully.\n"
     ]
    }
   ],
   "source": [
    "emotions = []\n",
    "for i, batch in enumerate(tqdm(batches, desc=\"Processing Batches\")):\n",
    "    try:\n",
    "        batch_result = goemotions(batch)\n",
    "        emotions.extend(batch_result)\n",
    "        print(f\"Batch {i+1}/{len(batches)} processed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove neutral emotion\n",
    "emotions = [text[:-1] for text in emotions]\n",
    "# Store scores for all emotions\n",
    "scores = [[emotion['score'] for emotion in sublist] for sublist in emotions]\n",
    "\n",
    "# Keep only k emotions per text\n",
    "k = 3\n",
    "ranked_emotions = [[emotions[i][k]['label'] for k in np.argsort(text)[::-1][:k]] for i, text in enumerate(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_maps = [chunk_to_text_mapping[i][-1] for i in range(len(chunk_to_text_mapping[:batch_size*10]))]\n",
    "df_emotions = pd.DataFrame({'Emotions': ranked_emotions, 'Plot ID': chunk_maps})\n",
    "grouped_emotions = df_emotions.groupby('Plot ID')['Emotions'].apply(np.sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data_top10.loc[:len(grouped_emotions),'Emotions'] = grouped_emotions['Emotions']\n",
    "genre_emotions = movie_data_top10.loc[:len(grouped_emotions)].groupby('Genre')['Emotions'].apply(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_top_emotions = {}\n",
    "for i, emotions in enumerate(genre_emotions):\n",
    "    if emotions == 0:\n",
    "        continue\n",
    "    genre = genre_emotions.index[i]\n",
    "    counts = Counter(emotions)\n",
    "    top_8 = counts.most_common(2)\n",
    "    em = [item[0] for item in top_8]\n",
    "    genre_top_emotions[genre] = em\n",
    "\n",
    "df_genre_top_emotions = pd.DataFrame({'Genre': list(genre_top_emotions.keys()), 'Emotions': genre_top_emotions.values()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
