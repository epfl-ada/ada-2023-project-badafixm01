{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for emotion score vector computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime, date, time\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a color blind friendly pallete\n",
    "CB_color_cycle = ['#377eb8','#ff7f00','#4daf4a',\n",
    "                  '#f781bf','#a65628','#984ea3',\n",
    "                  '#999999','#e41a1c','#dede00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xiaocheng's code for summary processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary(text):\n",
    "    \"\"\"\n",
    "    Tokenize, lemmatize, remove stopwords and punctuations from an input text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str, input text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str, preprocessed text\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    return \" \".join([lemmatizer.lemmatize(word.lower()) for word in text if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "summaries: dictionary, with movie_id as keys and list of preprocessed words in the summary as values\n",
    "\"\"\"\n",
    "with open(\"./test_data/plot_summaries.txt\", encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "original_summaries = [x.strip() for x in content] \n",
    "summaries = [preprocess_summary(d).split() for d in original_summaries]\n",
    "summaries = {summary[0]: summary[1:] for summary in summaries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Xiaocheng's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading the NRC lexicon emotion intensity data\n",
    "data = pd.read_table(\"NRC-lexicon/NRC-Emotion-Intensity-Lexicon-v1-ForVariousLanguages-withZeroIntensityEntries.txt\")\n",
    "data_filt = data.iloc[:,0:9].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the emotion intensity vector for each of the movies\n",
    "emo_vector = pd.DataFrame(columns=data_filt.columns)\n",
    "emo_vector = emo_vector.drop(columns = \"English Word\")\n",
    "\n",
    "for key in summaries:\n",
    " test = pd.DataFrame(columns=data_filt.columns)\n",
    " for i in range(0,len(summaries[key])):\n",
    "  selection = data_filt[data_filt[\"English Word\"] == summaries[key][i]]\n",
    "  test = pd.concat([test, selection], ignore_index=True)\n",
    " test[\"Movie ID\"] = key\n",
    " test = test.drop(columns = \"English Word\")\n",
    " test = test.set_index(\"Movie ID\")\n",
    " test = test.groupby(\"Movie ID\").sum()\n",
    " emo_vector = pd.concat([emo_vector, test], ignore_index=False)\n",
    "\n",
    "emo_vector.reset_index(inplace=True)\n",
    "emo_vector.rename(columns={\"index\": \"Wikipedia movie ID\"}, inplace=True)\n",
    "\n",
    "# Saving the new dataframe with the emotion vectors \n",
    "emo_vector.to_csv(\"MovieIDs_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie ID</th>\n",
       "      <th>Movie ID</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>23890098</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31186339</td>\n",
       "      <td>21.194</td>\n",
       "      <td>12.817</td>\n",
       "      <td>8.796</td>\n",
       "      <td>28.742</td>\n",
       "      <td>13.018</td>\n",
       "      <td>16.055</td>\n",
       "      <td>7.102</td>\n",
       "      <td>17.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20663735</td>\n",
       "      <td>12.641</td>\n",
       "      <td>6.514</td>\n",
       "      <td>3.917</td>\n",
       "      <td>14.270</td>\n",
       "      <td>10.557</td>\n",
       "      <td>11.749</td>\n",
       "      <td>3.398</td>\n",
       "      <td>17.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2231378</td>\n",
       "      <td>10.029</td>\n",
       "      <td>14.450</td>\n",
       "      <td>3.774</td>\n",
       "      <td>11.866</td>\n",
       "      <td>42.855</td>\n",
       "      <td>8.775</td>\n",
       "      <td>6.217</td>\n",
       "      <td>14.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>595909</td>\n",
       "      <td>6.694</td>\n",
       "      <td>7.516</td>\n",
       "      <td>2.769</td>\n",
       "      <td>7.930</td>\n",
       "      <td>8.273</td>\n",
       "      <td>8.365</td>\n",
       "      <td>2.320</td>\n",
       "      <td>15.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movie ID  Movie ID   anger  anticipation  disgust    fear     joy  sadness  \\\n",
       "0         0  23890098   0.545         0.508    0.000   0.000   0.514    0.000   \n",
       "1         1  31186339  21.194        12.817    8.796  28.742  13.018   16.055   \n",
       "2         2  20663735  12.641         6.514    3.917  14.270  10.557   11.749   \n",
       "3         3   2231378  10.029        14.450    3.774  11.866  42.855    8.775   \n",
       "4         4    595909   6.694         7.516    2.769   7.930   8.273    8.365   \n",
       "\n",
       "   surprise   trust  \n",
       "0     0.578   0.656  \n",
       "1     7.102  17.189  \n",
       "2     3.398  17.851  \n",
       "3     6.217  14.474  \n",
       "4     2.320  15.985  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_vector = pd.read_csv(\"MovieIDs_emotions.csv\")\n",
    "emo_vector.reset_index(inplace=True)\n",
    "emo_vector.rename(columns={\"index\": \"Movie ID\"}, inplace=True)\n",
    "emo_vector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Mya's code to parse the movie metadata:\n",
    "# Define the column names based on the metadata structure.\n",
    "column_names = [\n",
    "    \"Wikipedia movie ID\",\n",
    "    \"Freebase movie ID\",\n",
    "    \"Movie name\",\n",
    "    \"Movie release date\",\n",
    "    \"Movie box office revenue\",\n",
    "    \"Movie runtime\",\n",
    "    \"Movie languages\",\n",
    "    \"Movie countries\",\n",
    "    \"Movie genres\",\n",
    "]\n",
    "\n",
    "# Read the TSV file into a pandas DataFrame and specify that it's tab-separated.\n",
    "movie_md = pd.read_csv(\"test_data/movie.metadata.tsv\", sep='\\t', names=column_names, header=None) # <- Mya's code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Mya's code to extract genres:\n",
    "# Function to extract the genres\n",
    "def extract_genres(genre_data):\n",
    "    genre_names = []\n",
    "    pattern = r'\"([^\"]+)\"\\s*:\\s*\"([^\"]+)\"'\n",
    "    matches = re.findall(pattern, genre_data)\n",
    "    for match in matches:\n",
    "        genre_names.append(match[1])  # Extract the genre name\n",
    "    return ','.join(genre_names)\n",
    "\n",
    "# Apply the function to extract genre names\n",
    "movie_md[\"Movie genres\"] = movie_md[\"Movie genres\"].apply(extract_genres)\n",
    "movie_md['Movie genres'] = movie_md['Movie genres'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The column label 'Wikipedia movie ID' is not unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_466/2726467492.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;31m# Joinning the dataframes by 'Wikipedia movie ID' while removing all rows in movie_md for which we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# do not have plot summaries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0memo_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Movie ID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Wikipedia movie ID\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmeireles/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9839\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9840\u001b[0m     ) -> DataFrame:\n\u001b[1;32m   9841\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9843\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   9844\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9845\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9846\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmeireles/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 148\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmeireles/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    733\u001b[0m         (\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmeireles/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                         \u001b[0;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmeireles/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1788\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m                 \u001b[0mmulti_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mlabel_axis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"column\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0;34mf\"The {label_axis_name} label '{key}' is not unique.{multi_message}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             )\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The column label 'Wikipedia movie ID' is not unique."
     ]
    }
   ],
   "source": [
    "# Joinning the dataframes by 'Wikipedia movie ID' while removing all rows in movie_md for which we\n",
    "# do not have plot summaries.\n",
    "\n",
    "emo_vector.rename(columns={\"Movie ID\": \"Wikipedia movie ID\"}, inplace=True)\n",
    "\n",
    "df = emo_vector.merge(movie_md, on='Wikipedia movie ID', how='left')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows for which the release date is unknown\n",
    "df_filtered = df[df[\"Movie release date\"].notna()]\n",
    "len(df), len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling dates in the dataframe\n",
    "df_filtered[\"Movie release year\"] = df_filtered[\"Movie release date\"].apply(lambda x: parse(x).year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_filtered[\"Movie release year\"], df_filtered[\"anger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like someone was really ahead of their time! Let's see who were our movie pioneers\n",
    "df_filtered[df_filtered[\"Movie release year\"] < 1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By checking on the internet I saw that the correct release date is 2010, so I can just correct the dataframe\n",
    "df_filtered.loc[26305, \"Movie release date\"] = '2010-12-02'\n",
    "df_filtered.loc[26305, \"Movie release year\"] = 2010\n",
    "df_filtered.loc[26305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column with just the main genre of the movie\n",
    "df_filtered[\"Main genre\"] = df_filtered[\"Movie genres\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each genre and get the 10 most frequent genres\n",
    "top_10_genres = df_filtered['Main genre'].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Filter the DataFrame to keep only movies belonging to the top 10 genres\n",
    "df_topmg = df_filtered[df_filtered['Main genre'].isin(top_10_genres)]\n",
    "\n",
    "df_topmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the raw intensity scores per year for each on of the top 10 most common genres\n",
    "fig, ax = plt.subplots(10, 1, figsize=(20, 70))\n",
    "\n",
    "for j in range(len(top_10_genres)):\n",
    "    for i in range(1, 9):\n",
    "        sns.pointplot(\n",
    "            x=\"Movie release year\",\n",
    "            y=df_topmg.columns[i],\n",
    "            data=df_topmg[(df_topmg[\"Main genre\"] == top_10_genres[j])],\n",
    "            estimator=\"median\",\n",
    "            color=CB_color_cycle[i-1],\n",
    "            label=df_topmg.columns[i],\n",
    "            errorbar = None,\n",
    "            ax=ax[j]\n",
    "        )\n",
    "        \n",
    "        ax[j].legend(loc='upper right')\n",
    "        ax[j].set_title(top_10_genres[j])\n",
    "        ax[j].set_ylabel('Median Emotion Intensity Score')\n",
    "        ax[j].set_xlabel('Movie Release Year')\n",
    "        ax[j].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the emotion scores in percentage\n",
    "df_topmg_norm = df_topmg.copy()\n",
    "Total_score = df_topmg_norm[df_topmg_norm.columns[1:9]].sum(axis=1).copy()\n",
    "for i in range(1,9):\n",
    "    df_topmg_norm[df_topmg_norm.columns[i]] = df_topmg_norm[df_topmg_norm.columns[i]]*100/Total_score\n",
    "\n",
    "df_topmg_norm[df_topmg_norm.columns[1:9]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the normalized emotion scores per year for each on of the top 10 most common genres\n",
    "fig, ax = plt.subplots(10, 1, figsize=(20, 70), sharey = True)\n",
    "\n",
    "for j in range(len(top_10_genres)):\n",
    "    for i in range(1, 9):\n",
    "        sns.pointplot(\n",
    "            x=\"Movie release year\",\n",
    "            y=df_topmg_norm.columns[i],\n",
    "            data=df_topmg_norm[(df_topmg_norm[\"Main genre\"] == top_10_genres[j])],\n",
    "            estimator=\"median\",\n",
    "            color=CB_color_cycle[i-1],\n",
    "            label=df_topmg.columns[i],\n",
    "            #errorbar=('ci', 95),\n",
    "            errorbar = None,\n",
    "            ax=ax[j]\n",
    "        )\n",
    "        \n",
    "        ax[j].legend(loc='upper right')\n",
    "        ax[j].set_title(top_10_genres[j])\n",
    "        ax[j].set_ylabel('Median Emotion Intensity Score')\n",
    "        ax[j].set_xlabel('Movie Release Year')\n",
    "        ax[j].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
